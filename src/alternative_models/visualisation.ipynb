{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Visualisation\n",
    "\n",
    "We compare and visualise the performance of the bookmaker odds model in this repository and three alternatives:\n",
    "    - Bradley-Terry\n",
    "    - ELO\n",
    "    - Random Forest (catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDERS = ['bradleyterrymodel','elomodel','catboosttennismodel','bookmakeroddsmodel']\n",
    "NEAT_MODEL_NAME_MAP = {'bradleyterrymodel':'Bradley-Terry','elomodel':'ELO','catboosttennismodel':'Random Forest','bookmakeroddsmodel':'Bookmaker Model'}\n",
    "TOURNAMENTS = ['australian open','french open','wimbledon','us open']\n",
    "NEAT_TOURNAMENT_NAME_MAP = {'australian open':'Australian Open','french open':'French Open','wimbledon':'Wimbledon','us open':'US Open'}\n",
    "\n",
    "YEARS = [2024,2025]\n",
    "GENDERS = ['male','female']\n",
    "\n",
    "FILE_NAMES = [f\"{gender}_{tournament}_{year}.csv\" for gender in GENDERS for year in YEARS for tournament in TOURNAMENTS if not (tournament=='us open' and year == 2025) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Model Performance\n",
    "\n",
    "We calculate the accuracy, log-loss and Brier score of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_model_level_cis(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: list[str],\n",
    "    n_bootstrap: int = 10_000,\n",
    "    alpha: float = 0.1,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute model-level bootstrap confidence intervals for multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing model-level performance per tournament/year.\n",
    "        metrics: List of metric columns to bootstrap (e.g. ['brier_score', 'log_loss']).\n",
    "        n_bootstrap: Number of bootstrap resamples.\n",
    "        alpha: Significance level (0.05 gives 95% CIs).\n",
    "        random_state: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one row per model and columns for each metric's mean and CI bounds.\n",
    "        Column names are of the form <metric>_mean, <metric>_ci_lower, <metric>_ci_upper.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    results: list[dict[str, float]] = []\n",
    "\n",
    "    for model_name, df_model in df.groupby(\"model_name\"):\n",
    "        row_dict = {\"model_name\": model_name}\n",
    "\n",
    "        for metric in metrics:\n",
    "            values = df_model[metric].to_numpy()\n",
    "            n = len(values)\n",
    "\n",
    "            boot_means = np.empty(n_bootstrap)\n",
    "            for i in range(n_bootstrap):\n",
    "                sample = rng.choice(values, size=n, replace=True)\n",
    "                boot_means[i] = np.mean(sample)\n",
    "\n",
    "            row_dict[f\"{metric}_mean\"] = np.mean(values)\n",
    "            row_dict[f\"{metric}_ci_lower\"] = np.percentile(boot_means, 100 * alpha / 2)\n",
    "            row_dict[f\"{metric}_ci_upper\"] = np.percentile(boot_means, 100 * (1 - alpha / 2))\n",
    "\n",
    "        results.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model performance data\n",
    "model_performance_data:list[pd.Series] = []\n",
    "all_model_ci_data:list[pd.DataFrame] = []\n",
    "for model in MODEL_FOLDERS:\n",
    "    # Create an overall model dataframe\n",
    "    overall_model_dataframes:list[pd.DataFrame] = []\n",
    "    for file_name in FILE_NAMES:\n",
    "\n",
    "        # Load the data\n",
    "        model_tournament_data = pd.read_csv(f\"../../{model}/{file_name}\")\n",
    "\n",
    "        # Check all matches represented\n",
    "        assert len(model_tournament_data) == 127\n",
    "\n",
    "        # Calculate the metrics, adding them to the dataframe on a by-match level and calculating the tournament averages\n",
    "        model_tournament_data['log_loss'] = np.log(model_tournament_data['predicted_prob_winner'])\n",
    "        log_loss = (-1)*np.mean(model_tournament_data['log_loss'])\n",
    "        model_tournament_data['brier_score'] = np.square(1 - model_tournament_data['predicted_prob_winner'])\n",
    "        brier_score = np.mean(model_tournament_data['brier_score'])\n",
    "        model_tournament_data['accuracy'] = (model_tournament_data['predicted_prob_winner']>0.5) + 0.5*(model_tournament_data['predicted_prob_winner'] == 0.5)\n",
    "        accuracy = np.mean(model_tournament_data['accuracy'])\n",
    "\n",
    "        # Add the df to the overall model dataframes\n",
    "        model_tournament_data['model_name'] = NEAT_MODEL_NAME_MAP[model]\n",
    "        overall_model_dataframes.append(model_tournament_data)\n",
    "\n",
    "        # Create the series\n",
    "        male_data = file_name.split('_')[0] == 'male'\n",
    "        tournament = file_name.split('_')[1]\n",
    "        year = int(file_name.split('_')[-1].replace('.csv',''))\n",
    "\n",
    "        model_performance_data.append(\n",
    "            pd.Series(\n",
    "                {\n",
    "                    'male_data':male_data,\n",
    "                    'tournament':NEAT_TOURNAMENT_NAME_MAP[tournament],\n",
    "                    'year':year,\n",
    "                    'model_name':NEAT_MODEL_NAME_MAP[model],\n",
    "                    'accuracy':accuracy,\n",
    "                    'brier_score':brier_score,\n",
    "                    'log_loss':log_loss\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    # Calculate the CIs for the model\n",
    "    model_ci_data = bootstrap_model_level_cis(pd.concat(overall_model_dataframes),['accuracy','brier_score','log_loss'])\n",
    "    all_model_ci_data.append(model_ci_data)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bookmaker odds\n",
    "for file_name in FILE_NAMES:\n",
    "\n",
    "    # Get the tournament info from the name\n",
    "    male_data = file_name.split('_')[0] == 'male'\n",
    "    tournament = file_name.split('_')[1]\n",
    "    year = int(file_name.split('_')[-1].replace('.csv',''))\n",
    "    \n",
    "    # Create the gender suffix\n",
    "    gender_suffix = \"men\" if male_data else \"women\"\n",
    "    tournament_data = pd.read_csv(f\"../../src/main_package/data/{year}_{gender_suffix}.csv\")\n",
    "    tournament_data = tournament_data[tournament_data['Tournament'] == NEAT_TOURNAMENT_NAME_MAP[tournament]]\n",
    "\n",
    "    # Get the win probabilities\n",
    "    win_probabilities = (1/tournament_data['AvgW'])/((1/tournament_data['AvgW']) + (1/tournament_data['AvgL']))\n",
    "    \n",
    "\n",
    "    # Calculate the metrics, adding them to the dataframe on a by-match level and calculating the tournament averages\n",
    "    tournament_data['log_loss'] = np.log(win_probabilities)\n",
    "    log_loss = (-1)*np.mean(tournament_data['log_loss'])\n",
    "    tournament_data['brier_score'] = np.square(1 - win_probabilities)\n",
    "    brier_score = np.mean(tournament_data['brier_score'])\n",
    "    tournament_data['accuracy'] = (win_probabilities>0.5) + 0.5*(win_probabilities == 0.5)\n",
    "    accuracy = np.mean(tournament_data['accuracy'])\n",
    "\n",
    "    # Add the df to the overall model dataframes\n",
    "    tournament_data['model_name'] = \"Actual Odds\"\n",
    "    overall_model_dataframes.append(tournament_data)\n",
    "\n",
    "    # Create the series\n",
    "    male_data = file_name.split('_')[0] == 'male'\n",
    "    tournament = file_name.split('_')[1]\n",
    "    year = int(file_name.split('_')[-1].replace('.csv',''))\n",
    "\n",
    "    model_performance_data.append(\n",
    "        pd.Series(\n",
    "            {\n",
    "                'male_data':male_data,\n",
    "                'tournament':NEAT_TOURNAMENT_NAME_MAP[tournament],\n",
    "                'year':year,\n",
    "                'model_name': \"Actual Odds\",\n",
    "                'accuracy':accuracy,\n",
    "                'brier_score':brier_score,\n",
    "                'log_loss':log_loss\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "# Calculate the CIs for the model\n",
    "model_ci_data = bootstrap_model_level_cis(pd.concat(overall_model_dataframes),['accuracy','brier_score','log_loss'])\n",
    "all_model_ci_data.append(model_ci_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Model Performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>brier_score</th>\n",
       "      <th>log_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Odds</th>\n",
       "      <td>0.739314</td>\n",
       "      <td>0.174689</td>\n",
       "      <td>0.52342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bookmaker Model</th>\n",
       "      <td>0.723847</td>\n",
       "      <td>0.187689</td>\n",
       "      <td>0.562003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.704724</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>0.566656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELO</th>\n",
       "      <td>0.681102</td>\n",
       "      <td>0.198489</td>\n",
       "      <td>0.578891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bradley-Terry</th>\n",
       "      <td>0.702475</td>\n",
       "      <td>0.204035</td>\n",
       "      <td>0.623661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 accuracy brier_score  log_loss\n",
       "model_name                                     \n",
       "Actual Odds      0.739314    0.174689   0.52342\n",
       "Bookmaker Model  0.723847    0.187689  0.562003\n",
       "Random Forest    0.704724    0.193158  0.566656\n",
       "ELO              0.681102    0.198489  0.578891\n",
       "Bradley-Terry    0.702475    0.204035  0.623661"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Overall Model Performance:')\n",
    "overall_model_performance_data = pd.concat(model_performance_data,axis=1).T\n",
    "\n",
    "overall_model_performance_data.groupby('model_name')[['accuracy','brier_score','log_loss']].mean().sort_values(by='brier_score',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CI Model Performance\n",
      "        model_name  accuracy_mean  accuracy_ci_lower  accuracy_ci_upper  \\\n",
      "0    Bradley-Terry       0.702475           0.685039           0.719910   \n",
      "0              ELO       0.681102           0.663105           0.698819   \n",
      "0    Random Forest       0.704724           0.686727           0.722160   \n",
      "0  Bookmaker Model       0.723847           0.706412           0.741282   \n",
      "0      Actual Odds       0.739314           0.721879           0.756187   \n",
      "1  Bookmaker Model       0.723847           0.706412           0.741282   \n",
      "\n",
      "   brier_score_mean  brier_score_ci_lower  brier_score_ci_upper  \\\n",
      "0          0.204035              0.194903              0.213237   \n",
      "0          0.198489              0.192542              0.204550   \n",
      "0          0.193158              0.186501              0.199771   \n",
      "0          0.187689              0.180256              0.195274   \n",
      "0          0.174689              0.167488              0.181829   \n",
      "1          0.187689              0.180073              0.195313   \n",
      "\n",
      "   log_loss_mean  log_loss_ci_lower  log_loss_ci_upper  \n",
      "0      -0.623661          -0.652599          -0.596304  \n",
      "0      -0.578891          -0.593168          -0.564725  \n",
      "0      -0.566656          -0.582102          -0.551590  \n",
      "0      -0.562003          -0.581997          -0.542196  \n",
      "0      -0.523420          -0.540734          -0.506239  \n",
      "1      -0.562003          -0.582216          -0.542480  \n"
     ]
    }
   ],
   "source": [
    "print('CI Model Performance')\n",
    "print(pd.concat(all_model_ci_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
