{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison Visualisation\n",
    "\n",
    "We compare and visualise the performance of the bookmaker odds model in this repository and three alternatives:\n",
    "    - Bradley-Terry\n",
    "    - ELO\n",
    "    - Random Forest (catboost)\n",
    "\n",
    "Note: To run this notebook, you will need to run the alternative models, using either `run_alternative_models.ps1` (Windows) or `run_alternative_models.sh` (Bash - for Mac or Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDERS = ['bradleyterrymodel','elomodel','catboosttennismodel','bookmakeroddsmodel']\n",
    "NEAT_MODEL_NAME_MAP = {'bradleyterrymodel':'Bradley-Terry','elomodel':'ELO','catboosttennismodel':'Random Forest','bookmakeroddsmodel':'Bookmaker Model'}\n",
    "TOURNAMENTS = ['australian open','french open','wimbledon','us open']\n",
    "NEAT_TOURNAMENT_NAME_MAP = {'australian open':'Australian Open','french open':'French Open','wimbledon':'Wimbledon','us open':'US Open'}\n",
    "\n",
    "YEARS = [2024,2025]\n",
    "GENDERS = ['male','female']\n",
    "\n",
    "FILE_NAMES = [f\"{gender}_{tournament}_{year}.csv\" for gender in GENDERS for year in YEARS for tournament in TOURNAMENTS if not (tournament=='us open' and year == 2025) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Matches to be Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "combined_data = pd.concat([pd.read_csv(f\"../main_package/data/{year}_{gender}.csv\") for year in [2019,2020,2021,2022,2023,2024,2025] for gender in ['men','women']])\n",
    "combined_data['match_date'] = combined_data['Date'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%m/%d/%Y\")\n",
    "    )\n",
    "combined_data = combined_data.sort_values(by='match_date')\n",
    "\n",
    "combined_data['tournament_filter'] = combined_data['Tournament'].isin(['Australian Open','French Open','Wimbledon','US Open'])\n",
    "\n",
    "player_match_map = defaultdict(int)\n",
    "winner_cumulative_matches:list[int] = []\n",
    "loser_cumulative_matches:list[int] = []\n",
    "\n",
    "for _, match_datum in combined_data.iterrows():\n",
    "    winner_cumulative_matches.append(player_match_map[match_datum['Winner']])\n",
    "    loser_cumulative_matches.append(player_match_map[match_datum['Loser']])\n",
    "    if not match_datum['tournament_filter']:\n",
    "        player_match_map[match_datum['Winner']] += 1\n",
    "        player_match_map[match_datum['Loser']] += 1\n",
    "\n",
    "combined_data['cumulative_winner_matches'] =winner_cumulative_matches\n",
    "\n",
    "combined_data['cumulative_loser_matches'] = loser_cumulative_matches\n",
    "matches_removed = combined_data[(combined_data['tournament_filter'])&((combined_data['cumulative_winner_matches'] <= 5)|(combined_data['cumulative_loser_matches'] <= 5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Model Performance\n",
    "\n",
    "We calculate the accuracy, log-loss and Brier score of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_model_level_cis(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: list[str],\n",
    "    n_bootstrap: int = 10_000,\n",
    "    alpha: float = 0.05,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute model-level bootstrap confidence intervals for multiple metrics.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing model-level performance per tournament/year.\n",
    "        metrics: List of metric columns to bootstrap (e.g. ['brier_score', 'log_loss']).\n",
    "        n_bootstrap: Number of bootstrap resamples.\n",
    "        alpha: Significance level (0.05 gives 95% CIs).\n",
    "        random_state: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with one row per model and columns for each metric's mean and CI bounds.\n",
    "        Column names are of the form <metric>_mean, <metric>_ci_lower, <metric>_ci_upper.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    results: list[dict[str, float]] = []\n",
    "\n",
    "    for model_name, df_model in df.groupby(\"model_name\"):\n",
    "        row_dict = {\"model_name\": model_name}\n",
    "\n",
    "        for metric in metrics:\n",
    "            values = df_model[metric].to_numpy()\n",
    "            n = len(values)\n",
    "\n",
    "            boot_means = np.empty(n_bootstrap)\n",
    "            for i in range(n_bootstrap):\n",
    "                sample = rng.choice(values, size=n, replace=True)\n",
    "                boot_means[i] = np.mean(sample)\n",
    "\n",
    "            row_dict[f\"{metric}_mean\"] = np.mean(values)\n",
    "            row_dict[f\"{metric}_ci_lower\"] = np.percentile(boot_means, 100 * alpha / 2)\n",
    "            row_dict[f\"{metric}_ci_upper\"] = np.percentile(boot_means, 100 * (1 - alpha / 2))\n",
    "\n",
    "        results.append(row_dict)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model performance data\n",
    "model_performance_data:list[pd.Series] = []\n",
    "all_model_ci_data:list[pd.DataFrame] = []\n",
    "for model in MODEL_FOLDERS:\n",
    "    # Create an overall model dataframe\n",
    "    overall_model_dataframes:list[pd.DataFrame] = []\n",
    "    total_length = 0\n",
    "    for file_name in FILE_NAMES:\n",
    "\n",
    "        # Load the data\n",
    "        model_tournament_data = pd.read_csv(f\"../../{model}/{file_name}\")\n",
    "\n",
    "        # Check all matches represented\n",
    "        assert len(model_tournament_data) == 127\n",
    "\n",
    "        # Remove missing data matches\n",
    "        potential_missing_matches = matches_removed[(matches_removed['Tournament'] == NEAT_TOURNAMENT_NAME_MAP[file_name.split('_')[1]])&(matches_removed['Date'].apply(lambda x:int(x.split('/')[-1])) == int(file_name.split('_')[-1].replace('.csv',''))) ].reset_index(drop=True)\n",
    "        potential_missing_matches['remove'] = True\n",
    "        model_tournament_data = pd.merge(model_tournament_data, potential_missing_matches[['Winner','Loser','remove']], on = ['Winner','Loser'], how = 'left')\n",
    "        model_tournament_data = model_tournament_data[model_tournament_data['remove'].isna()].reset_index(drop=True)\n",
    "\n",
    "        # Calculate the metrics, adding them to the dataframe on a by-match level and calculating the tournament averages\n",
    "        model_tournament_data['log_loss'] = np.log(model_tournament_data['predicted_prob_winner'])\n",
    "        log_loss = (-1)*np.mean(model_tournament_data['log_loss'])\n",
    "        model_tournament_data['brier_score'] = np.square(1 - model_tournament_data['predicted_prob_winner'])\n",
    "        brier_score = np.mean(model_tournament_data['brier_score'])\n",
    "        model_tournament_data['accuracy'] = (model_tournament_data['predicted_prob_winner']>0.5) + 0.5*(model_tournament_data['predicted_prob_winner'] == 0.5)\n",
    "        accuracy = np.mean(model_tournament_data['accuracy'])\n",
    "\n",
    "        # Add the df to the overall model dataframes\n",
    "        model_tournament_data['model_name'] = NEAT_MODEL_NAME_MAP[model]\n",
    "        overall_model_dataframes.append(model_tournament_data)\n",
    "\n",
    "        # Create the series\n",
    "        male_data = file_name.split('_')[0] == 'male'\n",
    "        tournament = file_name.split('_')[1]\n",
    "        year = int(file_name.split('_')[-1].replace('.csv',''))\n",
    "\n",
    "        model_performance_data.append(\n",
    "            pd.Series(\n",
    "                {\n",
    "                    'male_data':male_data,\n",
    "                    'tournament':NEAT_TOURNAMENT_NAME_MAP[tournament],\n",
    "                    'year':year,\n",
    "                    'model_name':NEAT_MODEL_NAME_MAP[model],\n",
    "                    'accuracy':accuracy,\n",
    "                    'brier_score':brier_score,\n",
    "                    'log_loss':log_loss\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        total_length += len(model_tournament_data)\n",
    "    print(total_length)\n",
    "    # Calculate the CIs for the model\n",
    "    model_ci_data = bootstrap_model_level_cis(pd.concat(overall_model_dataframes),['accuracy','brier_score','log_loss'])\n",
    "    all_model_ci_data.append(model_ci_data)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bookmaker odds\n",
    "overall_model_dataframes:list[pd.DataFrame] = []\n",
    "for file_name in FILE_NAMES:\n",
    "\n",
    "    # Get the tournament info from the name\n",
    "    male_data = file_name.split('_')[0] == 'male'\n",
    "    tournament = file_name.split('_')[1]\n",
    "    year = int(file_name.split('_')[-1].replace('.csv',''))\n",
    "    \n",
    "    # Create the gender suffix\n",
    "    gender_suffix = \"men\" if male_data else \"women\"\n",
    "    tournament_data = pd.read_csv(f\"../../src/main_package/data/{year}_{gender_suffix}.csv\")\n",
    "    tournament_data = tournament_data[tournament_data['Tournament'] == NEAT_TOURNAMENT_NAME_MAP[tournament]]\n",
    "\n",
    "   # Remove missing data matches\n",
    "    potential_missing_matches = matches_removed[(matches_removed['Tournament'] == NEAT_TOURNAMENT_NAME_MAP[file_name.split('_')[1]])&(matches_removed['Date'].apply(lambda x:int(x.split('/')[-1])) == int(file_name.split('_')[-1].replace('.csv',''))) ].reset_index(drop=True)\n",
    "    potential_missing_matches['remove'] = True\n",
    "    tournament_data = pd.merge(tournament_data, potential_missing_matches[['Winner','Loser','remove']], on = ['Winner','Loser'], how = 'left')\n",
    "    tournament_data = tournament_data[tournament_data['remove'].isna()].reset_index(drop=True)\n",
    "\n",
    "    # Get the win probabilities\n",
    "    win_probabilities = (1/tournament_data['AvgW'])/((1/tournament_data['AvgW']) + (1/tournament_data['AvgL']))\n",
    "    \n",
    "\n",
    "    # Calculate the metrics, adding them to the dataframe on a by-match level and calculating the tournament averages\n",
    "    tournament_data['log_loss'] = np.log(win_probabilities)\n",
    "    log_loss = (-1)*np.mean(tournament_data['log_loss'])\n",
    "    tournament_data['brier_score'] = np.square(1 - win_probabilities)\n",
    "    brier_score = np.mean(tournament_data['brier_score'])\n",
    "    tournament_data['accuracy'] = (win_probabilities>0.5) + 0.5*(win_probabilities == 0.5)\n",
    "    accuracy = np.mean(tournament_data['accuracy'])\n",
    "\n",
    "    # Add the df to the overall model dataframes\n",
    "    tournament_data['model_name'] = \"Actual Odds\"\n",
    "    overall_model_dataframes.append(tournament_data)\n",
    "\n",
    "    # Create the series\n",
    "    male_data = file_name.split('_')[0] == 'male'\n",
    "    tournament = file_name.split('_')[1]\n",
    "    year = int(file_name.split('_')[-1].replace('.csv',''))\n",
    "\n",
    "    model_performance_data.append(\n",
    "        pd.Series(\n",
    "            {\n",
    "                'male_data':male_data,\n",
    "                'tournament':NEAT_TOURNAMENT_NAME_MAP[tournament],\n",
    "                'year':year,\n",
    "                'model_name': \"Actual Odds\",\n",
    "                'accuracy':accuracy,\n",
    "                'brier_score':brier_score,\n",
    "                'log_loss':log_loss\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "# Calculate the CIs for the model\n",
    "model_ci_data = bootstrap_model_level_cis(pd.concat(overall_model_dataframes),['accuracy','brier_score','log_loss'])\n",
    "all_model_ci_data.append(model_ci_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ci_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall Model Performance:')\n",
    "overall_model_performance_data = pd.concat(model_performance_data,axis=1).T\n",
    "\n",
    "overall_model_performance_data.groupby('model_name')[['accuracy','brier_score','log_loss']].mean().sort_values(by='brier_score',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CI Model Performance')\n",
    "print(pd.concat(all_model_ci_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
